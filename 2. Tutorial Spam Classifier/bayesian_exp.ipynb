{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNo795O3a1qwj1anAz4WNiF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eoGYxP2rFuxz","executionInfo":{"status":"ok","timestamp":1684249750187,"user_tz":-120,"elapsed":26723,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"12284688-5f10-4a01-d651-bb0e203d4335"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# Changing working directory\n","import os\n","os.chdir('/content/drive/My Drive/Tutorials/NLP_Learning/Udacity-NLP-tutorial/Udacity-Natural-Language-Processing-Nanodegree/2. Tutorial Spam Classifier')"]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"logQbBENF9AE","executionInfo":{"status":"ok","timestamp":1684249751434,"user_tz":-120,"elapsed":369,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"8ae4f790-8237-45ac-ac7b-06fba3a1150c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Tutorials/NLP_Learning/Udacity-NLP-tutorial/Udacity-Natural-Language-Processing-Nanodegree/2. Tutorial Spam Classifier\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXSUJm4_GBYc","executionInfo":{"status":"ok","timestamp":1684249755680,"user_tz":-120,"elapsed":6,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"99a60607-3c18-4a1d-f0ce-0c804ea93f80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bayesian_exp.ipynb\t  Bayesian_Inference_solution.ipynb  smsspamcollection\n","Bayesian_Inference.ipynb  images\n"]}]},{"cell_type":"code","source":["import os\n","import nltk\n","nltk.data.path.append(os.path.join(os.getcwd(), \"smsspamcollection\"))\n","import re"],"metadata":{"id":"gkCRX9ACGGdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xZbgH2EkHGqh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 0: Introduction to the Naive Bayes Theorem ###\n","\n","Bayes Theorem is one of the earliest probabilistic inference algorithms. It was developed by `Reverend Bayes` (which he used to try and infer the existence of God no less), and still performs extremely well for certain use cases. \n","\n","It's best to understand this theorem using an example. Let's say you are a member of the Secret Service and you have been deployed to protect the Democratic presidential nominee during one of his/her campaign speeches. Being a public event that is open to all, your job is not easy and you have to be on the constant lookout for threats. So one place to start is to put a certain threat-factor for each person. So based on the features of an individual, like age, whether the person is carrying a bag, looks nervous, etc., you can make a judgment call as to whether that person is a viable threat. \n","\n","If an individual ticks all the boxes up to a level where it crosses a threshold of doubt in your mind, you can take action and remove that person from the vicinity. \n","\n","_Bayes Theorem works in the same way, as we are computing the probability of an event (a person being a threat) based on the probabilities of certain related events (age, presence of bag or not, nervousness of the person, etc.)._\n","\n","One thing to consider is the independence of these features amongst each other. For example if a child looks nervous at the event then the likelihood of that person being a threat is not as much as say if it was a grown man who was nervous. To break this down a bit further, here there are two features we are considering, age AND nervousness. Say we look at these features individually, we could design a model that flags ALL persons that are nervous as potential threats. However, it is likely that we will have a lot of false positives as there is a strong chance that minors present at the event will be nervous. Hence by considering the age of a person along with the 'nervousness' feature we would definitely get a more accurate result as to who are potential threats and who aren't. \n","\n","This is the 'Naive' bit of the theorem where it considers each feature to be independent of each other which may not always be the case and hence that can affect the final judgement.\n","\n","In short, Bayes Theorem calculates the probability of a certain event happening (in our case, a message being spam) based on the joint probabilistic distributions of certain other events (in our case, the appearance of certain words in a message). We will dive into the workings of Bayes Theorem later in the mission, but first, let us understand the data we are going to work with."],"metadata":{"id":"hjqp2rAsHL57"}},{"cell_type":"code","source":["print(f'List of files inside \"smsspamcollection\" : ')\n","!ls smsspamcollection"],"metadata":{"id":"DnFCb-m9h_Zq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684257051700,"user_tz":-120,"elapsed":230,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"b6da6e81-4032-4d99-f7ff-bf3798b9ef49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["List of files inside \"smsspamcollection\" : \n","readme\tSMSSpamCollection\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import string"],"metadata":{"id":"fo3CRfjZisnf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"smsspamcollection/SMSSpamCollection\", sep='\\t', header = None, names=['label', 'sms_message'])"],"metadata":{"id":"RDdGq0WFi2r3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"id":"zfNbU9U3jF4I","executionInfo":{"status":"ok","timestamp":1684257165929,"user_tz":-120,"elapsed":322,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"2ae6c6a6-dd48-4908-ce6b-ce088b9dc565"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  label                                        sms_message\n","0   ham  Go until jurong point, crazy.. Available only ...\n","1   ham                      Ok lar... Joking wif u oni...\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","3   ham  U dun say so early hor... U c already then say...\n","4   ham  Nah I don't think he goes to usf, he lives aro..."],"text/html":["\n","  <div id=\"df-1e7838fa-5267-4f10-8779-5f111c189284\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>sms_message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e7838fa-5267-4f10-8779-5f111c189284')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1e7838fa-5267-4f10-8779-5f111c189284 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1e7838fa-5267-4f10-8779-5f111c189284');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# convert label to binary\n","df['label'] = df.label.map({\"ham\":0, \"spam\":1})"],"metadata":{"id":"LA0hUo1rjM-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"0SXvkXOBj20o","executionInfo":{"status":"ok","timestamp":1684257342075,"user_tz":-120,"elapsed":8,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"7b3d2a8b-113d-4668-96d3-a9d9e6c36a21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   label                                        sms_message\n","0      0  Go until jurong point, crazy.. Available only ...\n","1      0                      Ok lar... Joking wif u oni...\n","2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n","3      0  U dun say so early hor... U c already then say...\n","4      0  Nah I don't think he goes to usf, he lives aro..."],"text/html":["\n","  <div id=\"df-28d38410-20f3-4a29-b9f6-ca2efd5e7b2e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>sms_message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28d38410-20f3-4a29-b9f6-ca2efd5e7b2e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-28d38410-20f3-4a29-b9f6-ca2efd5e7b2e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-28d38410-20f3-4a29-b9f6-ca2efd5e7b2e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQWM3JK6j3-i","executionInfo":{"status":"ok","timestamp":1684257458381,"user_tz":-120,"elapsed":241,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"afe476a2-16f5-44a5-839e-1996b07afa88"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5572, 2)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":[],"metadata":{"id":"3unVkqfdkUVZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bag of Words\n","\n","What we have here in our data set is a large collection of text data (__5,572 rows of data__). Most ML algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy. \n","\n","Here we'd like to introduce the Bag of Words (`BoW`) concept which is a term used to specify the problems that have a 'bag of words' or a collection of text data that needs to be worked with. <font color='green'>\n","The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. \n","</font>\n","\n","It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter. \n","\n","Using a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word (token) being the column, and the corresponding (row, column) values being the frequency of occurrence of each word or token in that document.\n","\n","For example: \n","\n","Let's say we have 4 documents, which are text messages\n","in our case, as follows:\n","\n","`['Hello, how are you!',\n","'Win money, win from home.',\n","'Call me now',\n","'Hello, Call you tomorrow?']`\n","\n","Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.\n","\n","Let's break this down and see how we can do this conversion using a small set of documents.\n","\n","To handle this, we will be using sklearn's \n","[count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) method which does the following:\n","\n","* It tokenizes the string (separates the string into individual words) and gives an integer ID to each token.\n","* It counts the occurrence of each of those tokens.\n","\n","**Please Note:** \n","\n","* The `CountVectorizer` method automatically converts all tokenized words to their lower case form so that it does not treat words like 'He' and 'he' differently. It does this using the `lowercase` parameter which is by default set to `True`.\n","\n","* It also ignores all punctuation so that words followed by a punctuation mark (for example: 'hello!') are not treated differently than the same words not prefixed or suffixed by a punctuation mark (for example: 'hello'). It does this using the `token_pattern` parameter which has a default regular expression which selects tokens of 2 or more alphanumeric characters.\n","\n","* The third parameter to take note of is the `stop_words` parameter. Stop words refer to the most commonly used words in a language. They include words like 'am', 'an', 'and', 'the', etc. By setting this parameter value to `english`, CountVectorizer will automatically ignore all words (from our input text) that are found in the built in list of English stop words in scikit-learn. This is extremely helpful as stop words can skew our calculations when we are trying to find certain key words that are indicative of spam.\n","\n","We will dive into the application of each of these into our model in a later step, but for now it is important to be aware of such preprocessing techniques available to us when dealing with textual data."],"metadata":{"id":"r5UNvW_VkfQf"}},{"cell_type":"code","source":["# Convert all strings to lower case\n","documents = ['Hello, how are you!',\n","             'Win money, win from home.',\n","             'Call me now.',\n","             'Hello, Call hello you tomorrow?']\n","\n","l_docs = [w.lower() for w in documents]\n","print(f'documents to lower case : {l_docs}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTl9BhBDoc8b","executionInfo":{"status":"ok","timestamp":1684258889016,"user_tz":-120,"elapsed":4,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"4fa8643e-cf35-4b6e-f01c-e302dc88e08e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["documents to lower case : ['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vC8-WYXHpJha"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2: Removing all punctuation**\n","\n",">>**Instructions:**\n","Remove all punctuation from the strings in the document set. Save the strings into a list called \n","'sans_punctuation_documents'. "],"metadata":{"id":"Gvy1Itkrpkp0"}},{"cell_type":"code","source":["sans_punctuation_documents = []\n","\n","for doc in l_docs:\n","    sans_punctuation_documents.append(doc.translate(str.maketrans('','',string.punctuation)))\n","\n","print(f'after removal of punctuation : {sans_punctuation_documents}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZVzIsRJpnLn","executionInfo":{"status":"ok","timestamp":1684259011452,"user_tz":-120,"elapsed":7,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"5696605a-66f0-4010-9ee0-046eb4049e0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["after removal of punctuation : ['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"daAfIYaqqPg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3: Tokenization**\n","\n","Tokenizing a sentence in a document set means splitting up the sentence into individual words using a delimiter. The delimiter specifies what character we will use to identify the beginning and  end of a word. Most commonly, we use a single space as the delimiter character for identifying words, and this is true in our documents in this case also.\n","\n",">>**Instructions:**\n","Tokenize the strings stored in 'sans_punctuation_documents' using the split() method. Store the final document set \n","in a list called 'preprocessed_documents'."],"metadata":{"id":"tW4wZHTQqUu8"}},{"cell_type":"code","source":["preprocessed_documents = []\n","\n","# generating tokens\n","for tokens in sans_punctuation_documents:\n","    preprocessed_documents.append(tokens.split())\n","\n","print(f'generated tokens : {preprocessed_documents}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-SuZy8nqXsa","executionInfo":{"status":"ok","timestamp":1684259291858,"user_tz":-120,"elapsed":6,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"e215599a-5d8e-4ecb-b69d-48af0819ddde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["generated tokens : [['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cUygTX-0rUBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 4: Count frequencies**\n","\n","Now that we have our document set in the required format, we can proceed to counting the occurrence of each word in each document of the document set. We will use the `Counter` method from the Python `collections` library for this purpose. \n","\n","`Counter` counts the occurrence of each item in the list and returns a dictionary with the key as the item being counted and the corresponding value being the count of that item in the list. \n","\n",">>**Instructions:**\n","Using the Counter() method and preprocessed_documents as the input, create a dictionary with the keys being each word in each document and the corresponding values being the frequency of occurrence of that word. Save each Counter dictionary as an item in a list called 'frequency_list'."],"metadata":{"id":"FhqaoRAtrY7M"}},{"cell_type":"code","source":["from collections import Counter\n","\n","frequency_list = []\n","\n","for freq in preprocessed_documents:\n","    frequency_list.append(Counter(freq))\n","\n","print(f'frequencies are : {frequency_list}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"948-KZJvrciB","executionInfo":{"status":"ok","timestamp":1684259775954,"user_tz":-120,"elapsed":244,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"c38a5afb-b00e-463e-c606-a84977048fd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["frequencies are : [Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}), Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}), Counter({'call': 1, 'me': 1, 'now': 1}), Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PFFsieeVsZtu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='green'>\n","Congratulations! You have implemented the Bag of Words process from scratch! As we can see in our previous output, we have a frequency distribution dictionary which gives a clear view of the text that we are dealing with.\n","\n","We should now have a solid understanding of what is happening behind the scenes in the `sklearn.feature_extraction.text.CountVectorizer` method of scikit-learn. \n","\n","We will now implement `sklearn.feature_extraction.text.CountVectorizer` method in the next step.\n","</font>"],"metadata":{"id":"ebA7Rr9otTDc"}},{"cell_type":"code","source":[],"metadata":{"id":"3ak_s1zLtmWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Implementing Bag of Words in scikit-learn ###\n","\n","Now that we have implemented the BoW concept from scratch, let's go ahead and use scikit-learn to do this process in a clean and succinct way. We will use the same document set as we used in the previous step. \n","\n",">>**Instructions:**\n","Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. "],"metadata":{"id":"Ap0ck5vMtxkQ"}},{"cell_type":"code","source":["documents = ['Hello, how are you!',\n","                'Win money, win from home.',\n","                'Call me now.',\n","                'Hello, Call hello you tomorrow?']\n"],"metadata":{"id":"d_xPtbkUt2oq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vector = CountVectorizer()"],"metadata":{"id":"hZ9ECWlyt8FQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_vector.fit(documents)\n","print(f'count vector : {count_vector.get_feature_names_out()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"np9P9kZZuJD8","executionInfo":{"status":"ok","timestamp":1684262385178,"user_tz":-120,"elapsed":241,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"a30c7d9f-83d3-4c9c-bdf8-a5598b1234d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["count vector : ['are' 'call' 'from' 'hello' 'home' 'how' 'me' 'money' 'now' 'tomorrow'\n"," 'win' 'you']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"17y2N-Yk3Ehn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `get_feature_names()` method returns our feature names for this dataset, which is the set of words that make up our vocabulary for 'documents'."],"metadata":{"id":"vP0cLSTO3QIa"}},{"cell_type":"markdown","source":[">>**Instructions:**\n","Create a matrix with each row representing one of the 4 documents, and each column representing a word (feature name). \n","Each value in the matrix will represent the frequency of the word in that column occurring in the particular document in that row. \n","You can do this using the transform() method of CountVectorizer, passing in the document data set as the argument. The transform() method returns a matrix of NumPy integers, which you can convert to an array using\n","toarray(). Call the array 'doc_array'."],"metadata":{"id":"h01SsC1P3Nae"}},{"cell_type":"code","source":["doc_array = count_vector.transform(documents).toarray()\n","print(f'doc array is : \\n {doc_array}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-CQr0lBR3OcP","executionInfo":{"status":"ok","timestamp":1684262658091,"user_tz":-120,"elapsed":454,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"8de12441-670a-4f16-d8c9-17ef07487ee6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["doc array is : \n"," [[1 0 0 1 0 1 0 0 0 0 0 1]\n"," [0 0 1 0 1 0 0 1 0 0 2 0]\n"," [0 1 0 0 0 0 1 0 1 0 0 0]\n"," [0 1 0 2 0 0 0 0 0 1 0 1]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pSxIlLI54HXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we have a clean representation of the documents in terms of the frequency distribution of the words in them. To make it easier to understand our next step is to convert this array into a dataframe and name the columns appropriately.\n","\n",">>**Instructions:**\n","Convert the 'doc_array' we created into a dataframe, with the column names as the words (feature names). Call the dataframe 'frequency_matrix'."],"metadata":{"id":"IFmN9jf443dL"}},{"cell_type":"code","source":["frequency_matrix = pd.DataFrame(doc_array,columns=count_vector.get_feature_names_out())\n","\n","\n","\n","frequency_matrix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"y_jQj3rh46GF","executionInfo":{"status":"ok","timestamp":1684262996548,"user_tz":-120,"elapsed":214,"user":{"displayName":"NAVNEET","userId":"10106605363925002162"}},"outputId":"146e1eca-7513-40ac-9c0b-a6d5a6cd8c75"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n","0    1     0     0      1     0    1   0      0    0         0    0    1\n","1    0     0     1      0     1    0   0      1    0         0    2    0\n","2    0     1     0      0     0    0   1      0    1         0    0    0\n","3    0     1     0      2     0    0   0      0    0         1    0    1"],"text/html":["\n","  <div id=\"df-2071eb46-97c1-43d5-ac6c-8efb3067f204\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>call</th>\n","      <th>from</th>\n","      <th>hello</th>\n","      <th>home</th>\n","      <th>how</th>\n","      <th>me</th>\n","      <th>money</th>\n","      <th>now</th>\n","      <th>tomorrow</th>\n","      <th>win</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2071eb46-97c1-43d5-ac6c-8efb3067f204')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2071eb46-97c1-43d5-ac6c-8efb3067f204 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2071eb46-97c1-43d5-ac6c-8efb3067f204');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":[],"metadata":{"id":"wXLDi-ON5Jn_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Congratulations! You have successfully implemented a Bag of Words problem for a document dataset that we created. \n","\n","One potential issue that can arise from using this method is that if our dataset of text is extremely large (say if we have a large collection of news articles or email data), there will be certain values that are more common than others simply due to the structure of the language itself. For example, words like 'is', 'the', 'an', pronouns, grammatical constructs, etc., could skew our matrix and affect our analyis. \n","\n","There are a couple of ways to mitigate this. One way is to use the `stop_words` parameter and set its value to `english`. This will automatically ignore all the words in our input text that are found in a built-in list of English stop words in scikit-learn.\n","\n","Another way of mitigating this is by using the [tfidf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) method. This method is out of scope for the context of this lesson."],"metadata":{"id":"KiOfxYiq5kb-"}},{"cell_type":"code","source":[],"metadata":{"id":"LjJo4YNf5lYI"},"execution_count":null,"outputs":[]}]}